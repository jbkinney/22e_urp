{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 4: Pandas and TF binding site analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will start using Pandas. Pandas is the standard way of working with columnar data. However, there is a substantial learning curve. If you want to learn more about Pandas, here is a useful site: http://pandas.pydata.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will use Pandas to analyze transcription factor (TF) binding sites from *Escherichia coli*. We will first focus on CRP, a major regulator in *E. coli* with over 350 functional binding sites.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation and downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install palmerpenguins\n",
    "!pip -q install logomaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas is a library for working with tabular data. It was orignally based on the R data.frame library, but with a slightly different grammer and some different functionality. \n",
    "\n",
    "There are two main types of objects in Pandas.\n",
    "\n",
    "1)`Dataframe`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from palmerpenguins import load_penguins\n",
    "df = load_penguins()\n",
    "print(type(df))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A 2-D object\n",
    "* Has row (index) and column names\n",
    "* The orientation of rows vs columns matters a lot\n",
    "* Generally, you want features as columns and observations as rows\n",
    "* Features are variables, they are the things you measure, whether that be quantitatively or qualitatively. While observations are each data point, in this case it is each penguin\n",
    "\n",
    "2) `pd.Series`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series = df['species']\n",
    "print(type(series))\n",
    "series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A single column of data\n",
    "* Contains rownames but no column name, the rownames are always reffered to as `pd.Series.index`\n",
    "* Can have an attribute `pd.Series.name` that can serve as the column name\n",
    "* Works a lot like a python dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Things we need to learn \n",
    "1) How to create a pandas object/ read data in\n",
    "\n",
    "2) How to subset your data\n",
    "\n",
    "3) How to manipulate/mutate your data to create more data\n",
    "\n",
    "4) How to Summarize or aggregate your data\n",
    "\n",
    "\n",
    "\n",
    "## Preparing Code 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put this first\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Import logomaker; we will use this later for visualizing TF motifs.\n",
    "import logomaker "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will be analyzing a standing database of TF binding sites, which is available on RegulonDB. \n",
    "# Here is a command for downloading this file (this didn't work in lecture 1)\n",
    "!mkdir data \n",
    "!wget -O ./data/binding_site_db.txt http://regulondb.ccg.unam.mx/menu/download/datasets/files/BindingSiteSet.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what this database looks like\n",
    "!head -n 50 data/binding_site_db.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data\n",
    "\n",
    "When you store your data as a file you can use one of the `pd.read_*` functions to read in data from a variety of different file types\n",
    "\n",
    "Then to save your progress you can write using `pd.DataFrame.write_*`\n",
    "\n",
    "\n",
    "There are quite a lot of file types, and they all have their pros/cons. The simpliest and most commonly used one is `.csv`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To parse this file, use Pandas's method read_csv. \n",
    "df = pd.read_csv(\"data/binding_site_db.txt\", sep='\\t', comment='#', header=None)\n",
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check that the data has been properly loaded, call the method df.head()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You get the number of rows and columns from the attribute df.shape\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only want the TF name (column 1) and the TF binding site sequence (column 13)\n",
    "# To keep only these columns, index the df using a list of column names you want (in the order you want)\n",
    "col_names = [1,13]\n",
    "df = df[col_names]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data frames allow users to give columns meaningful names.\n",
    "# To rename the columns, set df.columns to a list of the desired names.\n",
    "df.columns = ['tf','site']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We see that some TF sites are listed as NaN. \n",
    "# Let's use the dropna() method to get rid of these rows.\n",
    "df = df.dropna()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that the last three modifications of df can be accomplished in one line \n",
    "df = pd.read_csv(\"data/binding_site_db.txt\", sep='\\t', comment='#',\n",
    "                 header=None, usecols=[1,13], names=['tf','site']).dropna()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out the pd.read_csv() documentation for a full list\n",
    "pd.read_csv?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe columns are called 'Series' objects. \n",
    "# Essentially, they're numpy arrays with some extra structure.\n",
    "col = df['tf']\n",
    "col.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can extract an element from a dataframe by using .loc[]\n",
    "df.loc[3,'site']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to generate sequence logos that represents the binding preferences of TFs in this database.  As a concrete example we'll use CRP, which has a well-characterized binding motif shown here:\n",
    "\n",
    "<img src=\"https://github.com/bharris12/URP_2021_Programming_Course/blob/main/lecture_3/data/crp_information_logo.png?raw=1\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "## Subsetting dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a TF\n",
    "tf = 'CRP'\n",
    "\n",
    "# Flag which rows in the dataframe have the correct TF name\n",
    "flags = (df['tf']==tf)\n",
    "flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab those rows. To be safe use copy() to make sure that, if we\n",
    "# alter tf_df, df itself doesn't change\n",
    "tf_df = df[flags].copy()\n",
    "tf_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting rows by a boolean vector, like `flags` is basically the only way I ever subset my rows. When you store rows as observations this is generally the case because you usually are **filtering** the rows based on a specific feature.\n",
    "\n",
    "### All the ways to subset DataFrames\n",
    "\n",
    "#### Rows\n",
    "By boolean : `df[BOOLEAN_VECTOR]` \n",
    "\n",
    "By name : `df.loc[LIST_OF_ROW_NAMES, :]`\n",
    "\n",
    "By Location : `df.iloc[LIST_OF_INTEGERS, :]`\n",
    "\n",
    "\n",
    "\n",
    "With a function `df.query('COLUMMN==value')` (This one is quite advanced, see [this tutorial for more](https://www.sharpsightlabs.com/blog/pandas-query/))\n",
    "#### Columns\n",
    "By boolean : `df.loc[:,BOOLEAN_VECTOR]`\n",
    "\n",
    "By name** : `df[LIST_OF_COLUMN_NAMES]`\n",
    "\n",
    "By Location : `df.iloc[:,LIST_OF_INTEGERS]`\n",
    "\n",
    "With a function : `df.filter(LOTS_OF_OPTIONS)` (This one is quite advanced, see [this tutorial for more](https://www.sharpsightlabs.com/blog/pandas-filter/))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each DNA binding site should be capitalized.\n",
    "# To do this, we reset 'site' column of tf_df\n",
    "\n",
    "# Get list of capitalized sites and replace the 'site' column with this\n",
    "capitalized_sites = [site.upper() for site in tf_df['site']]\n",
    "tf_df['site'] = capitalized_sites\n",
    "tf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to derive a motif, all sites we analyze need to be the same length.\n",
    "# It's good to check that this is actually the case.\n",
    "# We therefore add a column to tf_df listing the length of each site\n",
    "\n",
    "# Compute the length of each site and record this in a 'length' column\n",
    "site_lengths = [len(site) for site in tf_df['site']]\n",
    "tf_df['length'] = site_lengths\n",
    "tf_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises, part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**E4.1.** Use the `unique()` method to find how many different length exists for the TF binding sites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**E4.2.** Apparently these sites have a bunch of different lengths. For a better understanding of this, write a `for` loop to determine how many sites there are of each length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use value_counts() to see how many sites of each length there are. \n",
    "tf_df['length'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the mode() method to compute the most common binding site length\n",
    "# Note that mode() returns a tuple, of which we need to manually extract the first element\n",
    "length_mode = tf_df['length'].mode()[0]\n",
    "length_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag rows having sites of the chosen length\n",
    "flags = (tf_df['length']==length_mode)\n",
    "\n",
    "# Only keep these rows\n",
    "tf_df = tf_df[flags]\n",
    "tf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now extract the 'site' column from tf_df\n",
    "sites = tf_df['site']\n",
    "sites.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using logomaker.alignment_to_matrix function, compute the number of times each base occurs at each position\n",
    "# Note that this returns a dataframe\n",
    "counts_mat = logomaker.alignment_to_matrix(sites)\n",
    "counts_mat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This counts matrix can be visualized as a sequence logo\n",
    "logomaker.Logo(counts_mat,center_values=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#what we most commonly see in publications are actually \"information\" logos\n",
    "info_df = logomaker.transform_matrix(counts_mat,from_type='counts',to_type='information')\n",
    "logomaker.Logo(info_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises, part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**E4.3.** Fill out the function below so that the user can pass the name of any TF and get list of aligned sites back. Test that it works, e.g. on `tf='FNR'`, by getting a list of sites and making an information logo. Also test that it fails when it is supposed to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's turn this into a function \n",
    "def get_tf_sites(tf):\n",
    "   \n",
    "    # Load database\n",
    "    df = pd.read_csv(\"data/binding_site_db.txt\", sep='\\t', comment='#',\n",
    "                 header=None, usecols=[1,13], names=['tf','site']).dropna()\n",
    "    \n",
    "    # \n",
    "    # Fill in stuff here\n",
    "    # \n",
    "    \n",
    "    # Get sequence alignment and return it\n",
    "    return tf_df['site']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned_sites = get_tf_sites('FNR')\n",
    "#create information logo from retrived aligned sites of FNR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've went through the basics of dataframe and dataframe subsetting, lets go through a few pandas function that is commonly used for dataframe manipulation. For easiness, we'll do the following parts in the palmer penguine toy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "df = load_penguins()\n",
    "df = df.dropna()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pd.melt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how we plot the most common bar plot\n",
    "#You x axis is species, a categorical variable. \n",
    "#This means that we can use it to group/seperate the data by species \n",
    "sns.barplot(data=df, x='species', y='bill_length_mm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if I wanted to compare the distributions of different numerical variables. Say see how bill_length and bill_width compare (not within an observation)? \n",
    "\n",
    "To do this you need to make the data \"tall\" using the function pd.melt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bills_tall = pd.melt(df[['bill_length_mm', 'bill_depth_mm']])\n",
    "bills_tall.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bills_tall['variable'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=bills_tall,x='variable',y='value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, this is great, but you may notice that we have lost the information about which species each observation came from, when making the data tall, you can add another argument to melt that will bring with each value the species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bills_tall_species = pd.melt(\n",
    "    df[['bill_length_mm', 'bill_depth_mm', 'species']], id_vars='species', var_name='measurment',value_name='mm')\n",
    "bills_tall_species.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next lecture will be on data visulization! \n",
    "sns.barplot(data=bills_tall_species, x='measurment', y='mm', hue='species');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pd.concat()\n",
    "It is extremely common for data to come in separate files. But we need to join the files together. To use pd.concat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df_1 = df.iloc[:,0:4]\n",
    "sub_df_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df_2 = df.iloc[:,4:]\n",
    "sub_df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_df = pd.concat([sub_df_1, sub_df_2],axis=1)\n",
    "cat_df.head()\n",
    "#The axis=1 tells the function to stick the columns next to eachother, would hstack if we remove it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, just a few commonly used statistic tests. Here we'll heavily use the package `scipy`. Its a Python library that have implemented a lot of existing statistic tests.\n",
    "\n",
    "### Calculating Mann Whitney U test statistic:\n",
    " The Mann-Whitney U test is a nonparametric test of the null hypothesis that the distribution underlying sample x is the same as the distribution underlying sample y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "adelie_flag = df['species'] == 'Adelie'\n",
    "gentoo_flag = df['species'] == 'Gentoo'\n",
    "\n",
    "adelie_bill_l = df[adelie_flag]['bill_length_mm']\n",
    "gentoo_bill_l = df[gentoo_flag]['bill_length_mm']\n",
    "\n",
    "scipy.stats.mannwhitneyu(adelie_bill_l, gentoo_bill_l, nan_policy ='omit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating correlation coefficients\n",
    "The Pearson correlation coefficient measures the linear relationship between two datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_l = df['bill_length_mm']\n",
    "body_m = df['body_mass_g']\n",
    "scipy.stats.pearsonr(bill_l,body_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating P values\n",
    "\n",
    "We are not going into the complications around p-value. Here I'm just gonna simply show how to do the most simple t-test using python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.ttest_ind(adelie_bill_l, gentoo_bill_l, equal_var=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
